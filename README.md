[New!] Please also check [WAFT](https://github.com/princeton-vl/WAFT), our new efficient state-of-the-art method.

# SEA-RAFT

[[Paper](https://arxiv.org/abs/2405.14793)][[Slides](https://docs.google.com/presentation/d/1xZn-NowHuPqfdLDAaQwKyzYvP4HzGmT7/edit?usp=sharing&ouid=118125745783453356964&rtpof=true&sd=true)]

We introduce SEA-RAFT, a more simple, efficient, and accurate [RAFT](https://github.com/princeton-vl/RAFT) for optical flow. Compared with RAFT, SEA-RAFT is trained with a new loss (mixture of Laplace). It directly regresses an initial flow for faster convergence in iterative refinements and introduces rigid-motion pre-training to improve generalization. SEA-RAFT achieves state-of-the-art accuracy on the [Spring benchmark](https://spring-benchmark.org/) with a 3.69 endpoint-error (EPE) and a 0.36 1-pixel outlier rate (1px), representing 22.9\% and 17.8\% error reduction from best-published results. In addition, SEA-RAFT obtains the best cross-dataset generalization on KITTI and Spring. With its high efficiency, SEA-RAFT operates at least 2.3x faster than existing methods while maintaining competitive performance.

<img src="assets/visualization.png" width='1000'>

If you find SEA-RAFT useful for your work, please consider citing our academic paper:

<h3 align="center">
    <a href="https://arxiv.org/abs/2405.14793">
        SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow
    </a>
</h3>
<p align="center">
    <a href="https://memoryslices.github.io/">Yihan Wang</a>,
    <a href="https://www.lahavlipson.com/">Lahav Lipson</a>,
    <a href="http://www.cs.princeton.edu/~jiadeng">Jia Deng</a><br>
</p>

```
@article{wang2024sea,
  title={SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow},
  author={Wang, Yihan and Lipson, Lahav and Deng, Jia},
  journal={arXiv preprint arXiv:2405.14793},
  year={2024}
}
```

## Requirements
Our code is developed with pytorch 2.2.0, CUDA 12.2 and python 3.10.
```Shell
conda create --name SEA-RAFT python=3.10.13
conda activate SEA-RAFT
pip install -r requirements.txt
```

## Model Zoo

Google Drive: [link](https://drive.google.com/drive/folders/1YLovlvUW94vciWvTyLf-p3uWscbOQRWW?usp=sharing).

HuggingFace: [link](https://huggingface.co/papers/2405.14793).

## Custom Usage

We provide an example in `custom.py`. By default, this file will take two RGB images as the input and provide visualizations of the optical flow and the uncertainty. You can load your model by providing the path:
```Shell
python custom.py --cfg config/eval/spring-M.json --path models/Tartan-C-T-TSKH-spring540x960-M.pth
```
or load our models through HuggingFaceğŸ¤— (make sure you have installed huggingface-hub):
```Shell
python custom.py --cfg config/eval/spring-M.json --url MemorySlices/Tartan-C-T-TSKH-spring540x960-M
```

## Datasets
To evaluate/train SEA-RAFT, you will need to download the required datasets: [FlyingChairs](https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html#flyingchairs), [FlyingThings3D](https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html), [Sintel](http://sintel.is.tue.mpg.de/), [KITTI](http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=flow), [HD1K](http://hci-benchmark.iwr.uni-heidelberg.de/), [TartanAir](https://theairlab.org/tartanair-dataset/), and [Spring](https://spring-benchmark.org/).

By default `datasets.py` will search for the datasets in these locations. You can create symbolic links to wherever the datasets were downloaded in the `datasets` folder. Please check [RAFT](https://github.com/princeton-vl/RAFT) for more details.

```Shell
â”œâ”€â”€ datasets
    â”œâ”€â”€ Sintel
    â”œâ”€â”€ KITTI
    â”œâ”€â”€ FlyingChairs/FlyingChairs_release
    â”œâ”€â”€ FlyingThings3D
    â”œâ”€â”€ HD1K
    â”œâ”€â”€ spring
        â”œâ”€â”€ test
        â”œâ”€â”€ train
        â”œâ”€â”€ val
    â”œâ”€â”€ tartanair
```

## Training, Evaluation, and Submission

Please refer to [scripts/train.sh](scripts/train.sh), [scripts/eval.sh](scripts/eval.sh), and [scripts/submission.sh](scripts/submission.sh) for more details.

## å®æ—¶ä½å»¶è¿Ÿä¼ è¾“ä¸æ’å¸§ï¼ˆTailscale + WebRTCï¼‰

ä¸‹é¢æ˜¯æœ¬é¡¹ç›®åœ¨â€œè½¦ç«¯ â†’ ç”µè„‘ç«¯â€å®æ—¶ä¼ è¾“ GUI ç”»é¢å¹¶è¿›è¡Œä¸¢å¸§è¡¥å¸§çš„å®Œæ•´ä½¿ç”¨è¯´æ˜ã€‚

### åŠŸèƒ½æ¦‚è¿°
- è½¦ç«¯é‡‡é›†ç›¸æœºå¹¶æ¸²æŸ“ GUIï¼ˆHUD/Logo/é•œåƒï¼‰ï¼Œæœ¬åœ°æ˜¾ç¤ºåŒæ—¶é€šè¿‡ WebRTC å‘é€åˆ°ç”µè„‘ç«¯ã€‚
- é€šè¿‡ DataChannel å‘é€æ¯å¸§å…ƒæ•°æ®ï¼š`ts_ms`ã€`speed`ã€`steer`ã€`fps`ã€`width`ã€`height`ã€‚
- ç”µè„‘ç«¯æ£€æµ‹ä¸¢å¸§ï¼ˆåŸºäº `ts_ms` é—´éš”ï¼‰å¹¶ç”¨ SEA-RAFT é¢„æµ‹æ’å¸§ï¼Œé¢„æµ‹å¸§ä¼šæ˜¾ç¤º `PRED` æ ‡è®°ã€‚
- æ”¯æŒâ€œä»…æ˜¾ç¤ºä¸æ’å¸§â€çš„è°ƒè¯•æ¨¡å¼ã€‚

### ç›¸å…³è„šæœ¬
- è½¦ç«¯å‘é€ï¼š`webrtc_tailscale_realtime/car_gui_sender.py`
- ç”µè„‘ç«¯ä»…æ˜¾ç¤ºï¼ˆè°ƒè¯•ï¼‰ï¼š`webrtc_tailscale_realtime/receiver_viewer.py`
- ç”µè„‘ç«¯æ’å¸§ï¼š`webrtc_tailscale_realtime/receiver_realtime.py`

### ç¯å¢ƒå‡†å¤‡ï¼ˆç”µè„‘ç«¯ï¼‰
æ¨èåœ¨ `sea-raft` ç¯å¢ƒä¸­è¿è¡Œï¼Œå¹¶å›ºå®šä»¥ä¸‹ç‰ˆæœ¬ä»¥é¿å…ä¸å…¼å®¹é—®é¢˜ï¼š
- `numpy=1.26.4`
- `scipy=1.11.4`
- `opencv=4.8.1`ï¼ˆconda-forgeï¼‰

```Shell
conda activate sea-raft
conda install -y "numpy=1.26.4" "scipy=1.11.4"
conda install -y -c conda-forge "opencv=4.8.1"
pip install aiortc aiohttp av
```

### è½¦ç«¯æ“ä½œï¼ˆå‘é€ç«¯ï¼‰
1) ç¡®è®¤ Tailscale å·²è¿é€šï¼ˆèƒ½ ping é€šç”µè„‘ç«¯ IPï¼‰ã€‚
2) è¿è¡Œå‘é€è„šæœ¬ï¼š

```Shell
python3 webrtc_tailscale_realtime/car_gui_sender.py \
  --signal http://<PC_TAILSCALE_IP>:8080/offer \
  --send_fps 30 \
  --send_width 960 --send_height 540 \
  --codec vp8
```

å¸¸ç”¨å‚æ•°ï¼š
- `--send_fps`ï¼šå‘é€å¸§ç‡
- `--send_width/--send_height`ï¼šå‘é€åˆ†è¾¨ç‡ï¼ˆé™ä½å¯å‡å°‘å¸¦å®½å’Œå»¶è¿Ÿï¼‰
- `--codec`ï¼š`vp8`ï¼ˆå…¼å®¹å¥½ï¼‰æˆ– `h264`
- `--no_display`ï¼šè½¦ç«¯ä¸æ˜¾ç¤ºçª—å£ï¼ˆä»…å‘é€ï¼‰

è¯´æ˜ï¼š
- è½¦ç«¯æœ¬åœ°ä¼šæ˜¾ç¤º GUIï¼ˆé»˜è®¤ï¼‰ã€‚
- é€Ÿåº¦/æ–¹å‘ç›˜è§’åº¦æ¥è‡ª ROS2 è¯é¢˜ï¼ˆç¤ºä¾‹åœ¨è„šæœ¬å†…ï¼‰ï¼Œä¸å¯ç”¨æ—¶ä¼šç”¨ 0ã€‚

### ç”µè„‘ç«¯æ“ä½œï¼ˆä»…æ˜¾ç¤º/è°ƒè¯•ï¼‰
1) å¯åŠ¨è°ƒè¯•æ¥æ”¶å™¨ï¼ˆä¸åšæ’å¸§ï¼‰ï¼š

```Shell
conda run -n sea-raft python webrtc_tailscale_realtime/receiver_viewer.py \
  --listen 0.0.0.0 --port 8080
```

é¢„æœŸè¾“å‡ºï¼š
- ç»ˆç«¯çœ‹åˆ° `[viewer] track: video`
- ç»ˆç«¯æ¯ç§’æ‰“å° `[viewer] video fps ~ ...`
- çª—å£å‡ºç°å®æ—¶ç”»é¢

### ç”µè„‘ç«¯æ“ä½œï¼ˆæ’å¸§ï¼‰
1) å¯åŠ¨æ’å¸§æ¥æ”¶å™¨ï¼š

```Shell
conda run -n sea-raft python webrtc_tailscale_realtime/receiver_realtime.py \
  --listen 0.0.0.0 --port 8080 \
  --cfg config/eval/kitti-M.json \
  --path weight/Tartan-C-T-TSKH-kitti432x960-M.pth \
  --fps 30 --device cuda
```

æ’å¸§è§„åˆ™ï¼š
- ç”¨ `ts_ms` çš„é—´éš”åˆ¤æ–­æ˜¯å¦ä¸¢å¸§ï¼ˆé»˜è®¤é˜ˆå€¼ä¸º 1.5 å€å¸§é—´éš”ï¼‰ã€‚
- æ¯ 5 å¸§æ„å»ºä¸€æ¬¡é¢„æµ‹åºåˆ—ï¼ŒåŸºäºå‰ 2 å¸§æ¨å 8 å¸§ã€‚
- é¢„æµ‹æµä¼šæ ¹æ®é€Ÿåº¦æ¯”ä¾‹è¿›è¡Œç¼©æ”¾ï¼ˆé€Ÿåº¦è¶Šå¤§ï¼Œå…‰æµè¶Šé•¿ï¼‰ã€‚
- é¢„æµ‹å¸§æ˜¾ç¤º `PRED` æ ‡è®°ã€‚

å¸¸ç”¨å‚æ•°ï¼š
- `--fps`ï¼šæœŸæœ›å¸§ç‡ï¼ˆå½±å“ä¸¢å¸§åˆ¤æ–­ï¼‰
- `--gap_threshold`ï¼šä¸¢å¸§é˜ˆå€¼ç³»æ•°ï¼ˆé»˜è®¤ 1.5ï¼‰
- `--window_size`ï¼šæ„å»ºé¢„æµ‹åºåˆ—çš„çª—å£å¤§å°ï¼ˆé»˜è®¤ 5ï¼‰
- `--horizon`ï¼šæ¯æ¬¡é¢„æµ‹çš„å¸§æ•°ï¼ˆé»˜è®¤ 8ï¼‰
- `--raw_only`ï¼šåªæ˜¾ç¤ºï¼Œä¸æ’å¸§

### å¸¸è§é—®é¢˜
- çª—å£ä¸æ˜¾ç¤ºï¼šè¯·ç¡®è®¤ä¸æ˜¯åœ¨çº¯ SSH æ— å›¾å½¢ç¯å¢ƒï¼›éœ€è¦æœ¬åœ°æ¡Œé¢ç¯å¢ƒæˆ– X11 è½¬å‘ã€‚
- ç”»é¢ç©ºç™½ï¼šå…ˆç”¨ `receiver_viewer.py` éªŒè¯é“¾è·¯ï¼›è‹¥æ— è§†é¢‘å¸§ï¼Œæ£€æŸ¥è½¦ç«¯æ‘„åƒå¤´æ˜¯å¦æ‰“å¼€ã€‚
- æ€§èƒ½ä¸è¶³ï¼šé™ä½ `--send_width/--send_height` æˆ–å‘é€å¸§ç‡ã€‚

## Acknowledgements

This project relies on code from existing repositories: [RAFT](https://github.com/princeton-vl/RAFT), [unimatch](https://github.com/autonomousvision/unimatch/tree/master), [Flowformer](https://github.com/drinkingcoder/FlowFormer-Official), [ptlflow](https://github.com/hmorimitsu/ptlflow), and [LoFTR](https://github.com/zju3dv/LoFTR). We thank the original authors for their excellent work.
